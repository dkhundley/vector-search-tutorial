{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Data Retrieval\n",
    "Given that we will be testing vector search throughout these tutorials, we are going to need to play around with some data that an LLM was not originally trained on. That said, we'll be leveraging **Wikipedia** for this since it is constantly being updated with new information and also covers its content under a [Creative Commons license](https://en.wikipedia.org/wiki/Wikipedia:Copyrights). While I will be saving the information as a static CSV to this repo, there may come a point when you'll need to do a new pull of information just in case the information I'll be pulling is eventually baked into a new LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "\n",
    "In order to retrieve the data from Wikipedia, we're transparently going to replicate functionality [the same notebook in the OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb). I tried several options in order to not do a straight up rip off of what OpenAI did, but I found that most of them fell short of what I needed to do. That said, here are the special libraries that we will be using as part of this notebook:\n",
    "\n",
    "- `mwclient`: The \"MC\" standing for \"MediaWiki\", MediaWiki (if you're not aware) is sort of like the \"parent\" company overarching Wikipedia. If you were to consume their API directly (which you can do but is a pain), you'll likely find that documentation on a MediaWiki website. As this name implies, `mwclient` is the client we'll be using for downloading the Wikipedia articles.\n",
    "- `mwparserfromhell`: A neighboring library to `mwclient`, this library will specifically split the Wikipedia articles into their respective sections.\n",
    "\n",
    "Of course, these aren't the only libraries we'll be using, but you should already have familiarity with the others we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary Python libraries\n",
    "import pandas as pd\n",
    "import mwclient\n",
    "import mwparserfromhell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Information from a Single Article\n",
    "Just to ease ourselves into this process, let's demonstrate how to pull just a single article. At the time I am writing this, I am deep into the game **The Legend of Zelda: Tears of the Kingdom**, so we will be specifically grabbing the review (\"Reception\") information from that article. We'll execute all these cells in sequential order, but at a high level, here's what we're going to be doing:\n",
    "\n",
    "1. Connecting the `mwclient` to the appropriate website (which in our case is the \"vanilla\" Wikipedia).\n",
    "2. Retrieving the Wikipedia page by passing in the name of the article (in our case, \"The Legend of Zelda: Tears of the Kingdom\") into the `mwclient` instance.\n",
    "3. Retreiving the full Wikipedia text from the body of the page.\n",
    "4. Parsing the Wikipedia text with `mwparserfromhell`.\n",
    "5. Retrieving the section headings of the article to see if the article contains the section we're looking for. (Note: This isn't a big deal for our Zelda example since we know it's there, but we're going to want to programmatically check for this in every other article later on.)\n",
    "6. Retrieving the specific section we are looking for.\n",
    "7. Removing any \"code\" characters from the section text.\n",
    "8. Finally extracting the remaining text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting the mwclient to Wikipedia\n",
    "wikipedia_client = mwclient.Site('en.wikipedia.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the full text page from Wikipedia for Zelda\n",
    "zelda_wiki_text = wikipedia_client.pages['The Legend of Zelda: Tears of the Kingdom'].text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the text with mwparserfromhell\n",
    "parsed_wiki_text = mwparserfromhell.parse(zelda_wiki_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the section headings from the page\n",
    "section_headings = [str(heading) for heading in parsed_wiki_text.filter_headings()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if \"Reception\" is present in the Wikipedia article\n",
    "if '==Reception==' in section_headings:\n",
    "\n",
    "    # Retrieving all the sections in the parsed wiki text\n",
    "    all_sections = parsed_wiki_text.get_sections(levels = [2])\n",
    "\n",
    "    # Retreiving the appropriate section we need\n",
    "    for section in all_sections:\n",
    "        if str(section).startswith('==Reception=='):\n",
    "            reception_section = section\n",
    "\n",
    "    # Stripping out the code bits from the section text\n",
    "    reception_text = reception_section.strip_code()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Multiple Articles in a Single Category\n",
    "Now that we've demonstrated how to get text from a single article, we can make use of Wikipedia's \"Categories\" functionality to grab multiple articles from a given category. In the example below, I want to grab all the review (\"Reception\") text from any video game that launched in 2023. We will be making use of this page: [Category:2023 video games](https://en.wikipedia.org/wiki/Category:2023_video_games). Note that this list contains ALL video games launching in 2023, which means that there will be games on this list (e.g. Starfield, Spider-Man 2) as those games have not yet launched at the point of me doing this work. This is why we created our `if` statement in the cell above: if a \"Reception\" section is not present in the Wikipedia article, we'll simply skip it and move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from above that we simply passed in the name of the precise page associated to *The Legend of Zelda: Tears of the Kingdom* to obtain its text. In order to iterate through a category, we simply need to pass in that category name and iterate over that to obtain each article. Everything that we are working with in these libraries has a correlated data type, and Wikipedia pages are represented by this data type: `mwclient.page.Page`. This is important to keep in mind as categories may have subcategories under them, and we aren't interested in recursively gathering all those. In the OpenAI Cookbook notebook linked above, they do have some code that demonstrates that recursion, so check it out over there if you want to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting the mwclient to Wikipedia\n",
    "wikipedia_client = mwclient.Site('en.wikipedia.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting a list of everything under the 2023 video games Wikipedia category\n",
    "all_games = wikipedia_client.pages['Category:2023 video games']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating a list to hold the game reviews\n",
    "vg_reviews = []\n",
    "\n",
    "# Iterating over all the list of games\n",
    "for game in all_games:\n",
    "\n",
    "    # Checking to see if the instance is a Wikipedia page\n",
    "    if type(game) == mwclient.page.Page:\n",
    "\n",
    "        # Obtaining the name of the game as a string\n",
    "        game_name = game.name\n",
    "\n",
    "        # Getting the full text page from Wikipedia\n",
    "        wiki_text = wikipedia_client.pages[game_name].text()\n",
    "\n",
    "        # Parsing the text with mwparserfromhell\n",
    "        parsed_wiki_text = mwparserfromhell.parse(wiki_text)\n",
    "\n",
    "        # Retrieving the section headings from the page\n",
    "        section_headings = [str(heading) for heading in parsed_wiki_text.filter_headings()]\n",
    "\n",
    "        # Checking to see if \"Reception\" is present in the Wikipedia article\n",
    "        if '==Reception==' in section_headings:\n",
    "\n",
    "            # Retrieving all the sections in the parsed wiki text\n",
    "            all_sections = parsed_wiki_text.get_sections(levels = [2])\n",
    "\n",
    "            # Retreiving the appropriate section we need\n",
    "            for section in all_sections:\n",
    "                if str(section).startswith('==Reception=='):\n",
    "                    reception_section = section\n",
    "\n",
    "            # Stripping out the code bits from the section text\n",
    "            review_text = reception_section.strip_code()\n",
    "\n",
    "            # Appending the name of the game and review text to our collective list\n",
    "            vg_reviews.append([game_name, review_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've retrieved all our video game reviews as an array of arrays, we can seamlessly convert this into a Pandas DataFrame that we can later save out as a CSV file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishing a Pandas DataFrame to store our final results\n",
    "df_vg_reviews = pd.DataFrame(data = vg_reviews, columns = ['game_name', 'review_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_name</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Age of Wonders 4</td>\n",
       "      <td>Reception\\n\\nAge of Wonders 4 received a posit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aliens: Dark Descent</td>\n",
       "      <td>Reception\\n\\nAliens: Dark Descent received \"ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aquatico</td>\n",
       "      <td>Reception\\nThe game received mixed reviews upo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Atelier Ryza 3: Alchemist of the End &amp; the Sec...</td>\n",
       "      <td>Reception\\n\\nUpon release, Atelier Ryza 3 rece...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bayonetta Origins: Cereza and the Lost Demon</td>\n",
       "      <td>Reception\\n\\nBayonetta Origins: Cereza and the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           game_name  \\\n",
       "0                                   Age of Wonders 4   \n",
       "1                               Aliens: Dark Descent   \n",
       "2                                           Aquatico   \n",
       "3  Atelier Ryza 3: Alchemist of the End & the Sec...   \n",
       "4       Bayonetta Origins: Cereza and the Lost Demon   \n",
       "\n",
       "                                         review_text  \n",
       "0  Reception\\n\\nAge of Wonders 4 received a posit...  \n",
       "1  Reception\\n\\nAliens: Dark Descent received \"ge...  \n",
       "2  Reception\\nThe game received mixed reviews upo...  \n",
       "3  Reception\\n\\nUpon release, Atelier Ryza 3 rece...  \n",
       "4  Reception\\n\\nBayonetta Origins: Cereza and the...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the final results\n",
    "df_vg_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the reviews to file\n",
    "df_vg_reviews.to_csv('../data/2023-vg-reviews.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
